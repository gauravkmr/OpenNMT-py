Server: tesla
Raw Data Location:
    /data1/gauku/lowresource_mt/<LANG_CODE>/raw

Train, Valid, and Test Data Location:
    /data1/gauku/lowresource_mt/<LANG_CODE>/data

Preprocessed Data Location:
    /data1/gauku/lowresource_mt/<LANG_CODE>/data

Trained models:
    /data1/gauku/lowresource_mt/<LANG_CODE>/models



The below steps to get and preprocess Hindi-English data. Process is same for other languages.

STEPS:

#Getting data
1.  Login to tesla
2.  cd to the directory where you want to fetch files from ELISA server
3.  Login to ELISA sftp server
4.  Navigate to directory where file is located
5.  Copy the required files
    eg. to copy hin-eng files, run 'get elisa.hin*'


#Unzip the data files
cd /data1/gauku/lowresource_mt/hin/raw
tar xfz *.nomono.tgz
cd *.nomono
gunzip *.xml.gz


#Create conda env
conda create --name my_env python=3.6


#Extracting sentence pairs
1.  Clone OpenNMT-py, if not already done so
    git clone https://github.com/gauravkmr/OpenNMT-py.git
2.  cd OpenNMT-py
3.  git checkout bilingual
4.  source activate my_env
5.  pip install -r requirements.txt
6.  cd /scripts
7.  Run the below command to generate source target pairs
    #train
    python extract_data.py -lang hin -prefix train \
        -in /data1/gauku/lowresource_mt/hin/raw/elisa.hin.package.y3r1.v2.nomono/elisa.hin-eng.train.y3r1.v2.xml
        -tags "LRLP_TOKENIZED"
        -out /data1/gauku/lowresource_mt/hin/data/

    #test
    python extract_data.py -lang hin -prefix test \
        -in /data1/gauku/lowresource_mt/hin/raw/elisa.hin.package.y3r1.v2.nomono/elisa.hin-eng.test.y3r1.v2.xml
        -tags "LRLP_TOKENIZED"
        -out /data1/gauku/lowresource_mt/hin/data/

    #dev
    python extract_data.py -lang hin -prefix dev \
        -in /data1/gauku/lowresource_mt/hin/raw/elisa.hin.package.y3r1.v2.nomono/elisa.hin-eng.dev.y3r1.v2.xml
        -tags "LRLP_TOKENIZED"
        -out /data1/gauku/lowresource_mt/hin/data/


#Preprocess
python preprocess.py -train_src /data1/gauku/lowresource_mt/hin/data/hin_train_LRLP_TOKENIZED_SOURCE.txt \
                     -train_tgt /data1/gauku/lowresource_mt/hin/data/hin_train_LRLP_TOKENIZED_TARGET.txt \
                     -valid_src /data1/gauku/lowresource_mt/hin/data/hin_dev_LRLP_TOKENIZED_SOURCE.txt \
                     -valid_tgt /data1/gauku/lowresource_mt/hin/data/hin_dev_LRLP_TOKENIZED_TARGET.txt \
                     -save_data /data1/gauku/lowresource_mt/hin/data/hin

Note:
-save_data: last part ('hin' in this case) is used as filename prefix


Generated files:
/data1/gauku/lowresource_mt/hin/data/
        - hin.train.0.pt
        - hin.valid.0.pt
        - hin.vocab.pt



#Training
1. BASELINE RNN MODEL
python train.py -data /data1/gauku/lowresource_mt/hin/data/hin
                -save_model /data1/gauku/lowresource_mt/hin/models/hin_without_embeddings_baseline/hin_without_embedding
                -world_size 2
                -gpu_ranks 0 1

Usage:
-data: location of files generated by preprocess.py with the filename prefix ('hin' in this case)
-save_model: location where you want to save the generted model. The last part (after last '/') serves as model name prefix


2. BASELINE TRANSFORMER MODEL
python train.py -config config/config-transformer-base.yml




#Translate
python translate.py -model /data1/gauku/lowresource_mt/hin/models/hin_without_embeddings_baseline/hin_without_embedding_step_100000.pt
                    -src /data1/gauku/lowresource_mt/hin/data/hin_test_LRLP_TOKENIZED_SOURCE.txt
                    -output /data1/gauku/lowresource_mt/hin/data/hin_test_LRLP_TOKENIZED_MODELPRED.txt
                    -replace_unk -verbose